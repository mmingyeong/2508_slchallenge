{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5a1ccf-e526-4c7d-b8ca-860b0293bb39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T12:25:07.450931Z",
     "iopub.status.busy": "2025-09-05T12:25:07.450461Z",
     "iopub.status.idle": "2025-09-05T12:25:14.788555Z",
     "shell.execute_reply": "2025-09-05T12:25:14.787501Z",
     "shell.execute_reply.started": "2025-09-05T12:25:07.450895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]\n",
      "Torch : 2.6.0+cu124\n",
      "CUDA  : 12.4\n",
      "GPU   : Quadro RTX 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mmingyeong/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ imports via 'src.*' ready\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os, sys, importlib, time, json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch :\", torch.__version__)\n",
    "print(\"CUDA  :\", torch.version.cuda)\n",
    "print(\"GPU   :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n",
    "\n",
    "# 프로젝트 루트가 현재가 아니라면 아래처럼 경로 추가\n",
    "# sys.path.append(\"/path/to/your/project\")\n",
    "\n",
    "# 0) 패키지 세팅\n",
    "from pathlib import Path\n",
    "import sys, importlib\n",
    "\n",
    "PROJECT_ROOT = Path(\"/caefs/user/mmingyeong/2508_slchallence\")\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# 2) 올바른 임포트 (패키지 경로 사용)\n",
    "import utils as utils\n",
    "import model as model\n",
    "import data_loader as data_loader\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(model)\n",
    "importlib.reload(data_loader)\n",
    "\n",
    "from src.model import convnextv2_atto, convnextv2_nano, convnextv2_tiny\n",
    "from src.data_loader import get_dataloaders, LensFITSBinaryDataset\n",
    "\n",
    "print(\"✅ imports via 'src.*' ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79c36f4-4b91-4c9b-892a-e016106d20d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T12:25:14.790357Z",
     "iopub.status.busy": "2025-09-05T12:25:14.789669Z",
     "iopub.status.idle": "2025-09-05T12:25:14.795200Z",
     "shell.execute_reply": "2025-09-05T12:25:14.794274Z",
     "shell.execute_reply.started": "2025-09-05T12:25:14.790320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import convnextv2_atto, convnextv2_nano, convnextv2_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbcbad5-2a8a-482b-94c5-40e2529816e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T12:25:14.797344Z",
     "iopub.status.busy": "2025-09-05T12:25:14.796941Z",
     "iopub.status.idle": "2025-09-05T12:25:15.384227Z",
     "shell.execute_reply": "2025-09-05T12:25:15.383410Z",
     "shell.execute_reply.started": "2025-09-05T12:25:14.797315Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import types, sys, os, json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# 프로젝트 경로\n",
    "# -----------------------\n",
    "PROJECT_ROOT = Path(\"/caefs/user/mmingyeong/2508_slchallence\")\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import predict\n",
    "import evaluate\n",
    "\n",
    "# -----------------------\n",
    "# 데이터 경로\n",
    "# -----------------------\n",
    "SLSIM_LENSES_DIR      = \"/caefs/data/IllustrisTNG/slchallenge/slsim_lenses/slsim_lenses\"\n",
    "SLSIM_NONLENSES_DIR   = \"/caefs/data/IllustrisTNG/slchallenge/slsim_nonlenses/slsim_nonlenses\"\n",
    "HSC_DEG_LENSES_DIR    = \"/caefs/data/IllustrisTNG/slchallenge/hsc_lenses/hsc_lenses\"\n",
    "HSC_DEG_NONLENSES_DIR = \"/caefs/data/IllustrisTNG/slchallenge/hsc_nonlenses/hsc_nonlenses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81bde37-9a6f-476b-aae9-d7e485fd7d03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T12:25:15.385199Z",
     "iopub.status.busy": "2025-09-05T12:25:15.384966Z",
     "iopub.status.idle": "2025-09-05T12:25:15.490158Z",
     "shell.execute_reply": "2025-09-05T12:25:15.489037Z",
     "shell.execute_reply.started": "2025-09-05T12:25:15.385184Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Optuna wrapper that calls your existing train.py (super simple)\n",
    "import os, csv, time, shutil, types, optuna, math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import train as train_mod   # <-- uses your src/train.py already on sys.path\n",
    "\n",
    "# === USER-PROVIDED PATHS must exist as variables in your notebook ===\n",
    "# SLSIM_LENSES_DIR, SLSIM_NONLENSES_DIR, HSC_DEG_LENSES_DIR, HSC_DEG_NONLENSES_DIR\n",
    "\n",
    "def _mk_args_for_train(trial, save_dir, *, \n",
    "                       arch, lr, batch, weight_decay, drop_path,\n",
    "                       epochs=20, patience=8, seed=42,\n",
    "                       take_train_frac=0.10, take_val_fraction=0.20,\n",
    "                       num_workers=8):\n",
    "    \"\"\"Build an argparse-like object for train.main().\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ns = types.SimpleNamespace(\n",
    "        # data dirs\n",
    "        slsim_lenses=SLSIM_LENSES_DIR,\n",
    "        slsim_nonlenses=SLSIM_NONLENSES_DIR,\n",
    "        hsc_lenses=HSC_DEG_LENSES_DIR,\n",
    "        hsc_nonlenses=HSC_DEG_NONLENSES_DIR,\n",
    "        # dataloader\n",
    "        batch_size=batch,\n",
    "        num_workers=num_workers,\n",
    "        no_augment=True,                 # HPO에는 증강 끄고 고정\n",
    "        take_train_frac=take_train_frac, # 5–10% sweep\n",
    "        take_val_fraction=take_val_fraction,\n",
    "        take_test_fraction=None,\n",
    "        # split\n",
    "        train_frac=0.70, val_frac=0.15, test_frac=0.15,\n",
    "        # model/optim\n",
    "        model_size=arch, drop_path=drop_path,\n",
    "        lr=lr, weight_decay=weight_decay,\n",
    "        cosine=False, warmup_epochs=0,\n",
    "        # train\n",
    "        epochs=epochs, patience=patience, min_delta=0.0,\n",
    "        seed=seed, device=device, log_every=200,\n",
    "        # save\n",
    "        save_dir=save_dir,\n",
    "        # preprocessing toggles: your plan = Gaussian + normalization, no padding\n",
    "        apply_padding=False,\n",
    "        out_size_when_padded=64,\n",
    "        apply_normalization=True,\n",
    "        clip_q=0.997,\n",
    "        low_clip_q=None,\n",
    "        use_mad=False,\n",
    "        # smoothing\n",
    "        smoothing_mode=\"gaussian\",\n",
    "        gaussian_sigma=1.0,\n",
    "        guided_radius=2,\n",
    "        guided_eps=1e-2,\n",
    "    )\n",
    "    return ns\n",
    "\n",
    "def _read_best_val_auc(csv_path: str) -> float:\n",
    "    \"\"\"Parse training_log.csv and return max val_auc (ignoring NaN).\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return float(\"nan\")\n",
    "    best = float(\"-inf\")\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                v = float(row[\"val_auc\"])\n",
    "                if math.isfinite(v) and v > best:\n",
    "                    best = v\n",
    "            except Exception:\n",
    "                continue\n",
    "    return best if best != float(\"-inf\") else float(\"nan\")\n",
    "\n",
    "def run_hpo_with_train(\n",
    "    n_trials=8,\n",
    "    epochs=20,\n",
    "    patience=8,\n",
    "    take_train_frac=0.10,\n",
    "    take_val_fraction=0.20,   # 검증도 20%만 사용해서 속도 ↑ (원하면 None)\n",
    "    seed=42,\n",
    "    num_workers=8,\n",
    "    out_root=\"./optuna_runs\",\n",
    "):\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        # --- minimal search space (matches your CLI) ---\n",
    "        arch = trial.suggest_categorical(\"arch\", [\"atto\", \"nano\", \"tiny\"])\n",
    "        lr   = trial.suggest_float(\"lr\", 3e-4, 1e-3, log=True)\n",
    "        batch= trial.suggest_categorical(\"batch_size\", [128, 256])\n",
    "        wd   = trial.suggest_float(\"weight_decay\", 3e-5, 3e-4, log=True)\n",
    "        dp   = trial.suggest_float(\"drop_path\", 0.0, 0.10)\n",
    "\n",
    "        save_dir = os.path.join(out_root, f\"trial_{trial.number:03d}_{arch}_bs{batch}\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        args = _mk_args_for_train(\n",
    "            trial, save_dir,\n",
    "            arch=arch, lr=lr, batch=batch,\n",
    "            weight_decay=wd, drop_path=dp,\n",
    "            epochs=epochs, patience=patience, seed=seed,\n",
    "            take_train_frac=take_train_frac,\n",
    "            take_val_fraction=take_val_fraction,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "        # --- train using your train.main() ---\n",
    "        t0 = time.time()\n",
    "        train_mod.main(args)\n",
    "        dur = time.time() - t0\n",
    "\n",
    "        # --- read best val AUC from training_log.csv ---\n",
    "        csv_path = os.path.join(save_dir, \"training_log.csv\")\n",
    "        best_val_auc = _read_best_val_auc(csv_path)\n",
    "        if not (best_val_auc == best_val_auc):  # NaN check\n",
    "            best_val_auc = 0.5\n",
    "\n",
    "        # attach extras for later inspection\n",
    "        trial.set_user_attr(\"save_dir\", save_dir)\n",
    "        trial.set_user_attr(\"duration_sec\", int(dur))\n",
    "        return float(best_val_auc)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")  # in-memory\n",
    "    print(\"[Optuna] starting fresh in-memory study\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"\\n=== HPO finished ===\")\n",
    "    print(f\"Best AUC: {study.best_value:.6f}\")\n",
    "    print(\"Best params:\", study.best_trial.params)\n",
    "    print(\"Artifacts in:\", study.best_trial.user_attrs[\"save_dir\"])\n",
    "\n",
    "    return study\n",
    "\n",
    "# ---- EXAMPLE RUN ----\n",
    "# study = run_hpo_with_train(\n",
    "#     n_trials=4,\n",
    "#     epochs=20, patience=8,\n",
    "#     take_train_frac=0.10,   # 10% of train only\n",
    "#     take_val_fraction=0.20, # 20% of val only (speed)\n",
    "#     num_workers=8,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ed0d0c-9d84-4010-954f-5a4f58f02f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T12:25:15.490878Z",
     "iopub.status.busy": "2025-09-05T12:25:15.490708Z",
     "iopub.status.idle": "2025-09-05T12:33:23.230928Z",
     "shell.execute_reply": "2025-09-05T12:33:23.229795Z",
     "shell.execute_reply.started": "2025-09-05T12:25:15.490864Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-05 21:25:15,492] A new study created in memory with name: no-name-dad64b6b-ec31-4bd2-a64d-cc202f9b9721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optuna] starting fresh in-memory study\n",
      "2025-09-05 21:25:15 [INFO] [train] Logger initialized -> ./optuna_runs/trial_000_nano_bs256/train.log\n",
      "2025-09-05 21:25:15 [INFO] [train] 🚀 Configuration\n",
      "2025-09-05 21:25:15 [INFO] [train]   slsim_lenses: /caefs/data/IllustrisTNG/slchallenge/slsim_lenses/slsim_lenses\n",
      "2025-09-05 21:25:15 [INFO] [train]   slsim_nonlenses: /caefs/data/IllustrisTNG/slchallenge/slsim_nonlenses/slsim_nonlenses\n",
      "2025-09-05 21:25:15 [INFO] [train]   hsc_lenses: /caefs/data/IllustrisTNG/slchallenge/hsc_lenses/hsc_lenses\n",
      "2025-09-05 21:25:15 [INFO] [train]   hsc_nonlenses: /caefs/data/IllustrisTNG/slchallenge/hsc_nonlenses/hsc_nonlenses\n",
      "2025-09-05 21:25:15 [INFO] [train]   batch_size: 256\n",
      "2025-09-05 21:25:15 [INFO] [train]   num_workers: 8\n",
      "2025-09-05 21:25:15 [INFO] [train]   no_augment: True\n",
      "2025-09-05 21:25:15 [INFO] [train]   take_train_frac: 0.1\n",
      "2025-09-05 21:25:15 [INFO] [train]   take_val_fraction: 0.2\n",
      "2025-09-05 21:25:15 [INFO] [train]   take_test_fraction: None\n",
      "2025-09-05 21:25:15 [INFO] [train]   train_frac: 0.7\n",
      "2025-09-05 21:25:15 [INFO] [train]   val_frac: 0.15\n",
      "2025-09-05 21:25:15 [INFO] [train]   test_frac: 0.15\n",
      "2025-09-05 21:25:15 [INFO] [train]   model_size: nano\n",
      "2025-09-05 21:25:15 [INFO] [train]   drop_path: 0.0022994550074793008\n",
      "2025-09-05 21:25:15 [INFO] [train]   lr: 0.0005358804106038669\n",
      "2025-09-05 21:25:15 [INFO] [train]   weight_decay: 5.6345802047845344e-05\n",
      "2025-09-05 21:25:15 [INFO] [train]   cosine: False\n",
      "2025-09-05 21:25:15 [INFO] [train]   warmup_epochs: 0\n",
      "2025-09-05 21:25:15 [INFO] [train]   epochs: 20\n",
      "2025-09-05 21:25:15 [INFO] [train]   patience: 8\n",
      "2025-09-05 21:25:15 [INFO] [train]   min_delta: 0.0\n",
      "2025-09-05 21:25:15 [INFO] [train]   seed: 42\n",
      "2025-09-05 21:25:15 [INFO] [train]   device: cuda\n",
      "2025-09-05 21:25:15 [INFO] [train]   log_every: 200\n",
      "2025-09-05 21:25:15 [INFO] [train]   save_dir: ./optuna_runs/trial_000_nano_bs256\n",
      "2025-09-05 21:25:15 [INFO] [train]   apply_padding: False\n",
      "2025-09-05 21:25:15 [INFO] [train]   out_size_when_padded: 64\n",
      "2025-09-05 21:25:15 [INFO] [train]   apply_normalization: True\n",
      "2025-09-05 21:25:15 [INFO] [train]   clip_q: 0.997\n",
      "2025-09-05 21:25:15 [INFO] [train]   low_clip_q: None\n",
      "2025-09-05 21:25:15 [INFO] [train]   use_mad: False\n",
      "2025-09-05 21:25:15 [INFO] [train]   smoothing_mode: gaussian\n",
      "2025-09-05 21:25:15 [INFO] [train]   gaussian_sigma: 1.0\n",
      "2025-09-05 21:25:15 [INFO] [train]   guided_radius: 2\n",
      "2025-09-05 21:25:15 [INFO] [train]   guided_eps: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-05 21:31:01 [INFO] [train] ✅ Epoch 1: best model updated (val_loss=0.503336)\n",
      "2025-09-05 21:31:01 [INFO] [train] 📉 Epoch 001/20 | Train Loss 0.6140 Acc 68.06% | Val Loss 0.5033 Acc 76.28% AUC 0.8453 | LR 5.36e-04 | 342.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-09-05 21:33:22,432] Trial 0 failed with parameters: {'arch': 'nano', 'lr': 0.0005358804106038669, 'batch_size': 256, 'weight_decay': 5.6345802047845344e-05, 'drop_path': 0.0022994550074793008} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/users/mmingyeong/.local/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_116667/712984078.py\", line 108, in objective\n",
      "    train_mod.main(args)\n",
      "  File \"/caefs/user/mmingyeong/2508_slchallence/src/train.py\", line 308, in main\n",
      "    train_loss, train_acc = train_one_epoch(\n",
      "                            ^^^^^^^^^^^^^^^^\n",
      "  File \"/caefs/user/mmingyeong/2508_slchallence/src/train.py\", line 141, in train_one_epoch\n",
      "    for i, batch in enumerate(pbar, 1):\n",
      "  File \"/home/users/mmingyeong/.local/lib/python3.12/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1458, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1410, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1251, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/queue.py\", line 180, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/threading.py\", line 338, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-09-05 21:33:22,445] Trial 0 failed with value None.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_116667/3812415881.py\", line 1, in <module>\n",
      "    study = run_hpo_with_train(\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_116667/712984078.py\", line 124, in run_hpo_with_train\n",
      "    study.optimize(objective, n_trials=n_trials)\n",
      "  File \"/home/users/mmingyeong/.local/lib/python3.12/site-packages/optuna/study/study.py\", line 490, in optimize\n",
      "    _optimize(\n",
      "  File \"/home/users/mmingyeong/.local/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/home/users/mmingyeong/.local/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
      "    frozen_trial_id = _run_trial(study, func, catch)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/users/mmingyeong/.local/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 258, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/home/users/mmingyeong/.local/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_116667/712984078.py\", line 108, in objective\n",
      "    train_mod.main(args)\n",
      "  File \"/caefs/user/mmingyeong/2508_slchallence/src/train.py\", line 308, in main\n",
      "    train_loss, train_acc = train_one_epoch(\n",
      "                            ^^^^^^^^^^^^^^^^\n",
      "  File \"/caefs/user/mmingyeong/2508_slchallence/src/train.py\", line 141, in train_one_epoch\n",
      "    for i, batch in enumerate(pbar, 1):\n",
      "  File \"/home/users/mmingyeong/.local/lib/python3.12/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1458, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1410, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1251, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/queue.py\", line 180, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/threading.py\", line 338, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2176, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1182, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1053, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 861, in structured_traceback\n",
      "    formatted_exceptions: list[list[str]] = self.format_exception_as_a_whole(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 746, in format_exception_as_a_whole\n",
      "    records = self.get_records(etb, context, tb_offset) if etb else []\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 848, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/stack_data/core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/stack_data/utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/stack_data/core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/stack_data/core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/executing/executing.py\", line 264, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/executing/executing.py\", line 183, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/executing/executing.py\", line 202, in for_filename\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/linecache.py\", line 72, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "SystemError: <built-in function _error_if_any_worker_fails> returned a result with an exception set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    }
   ],
   "source": [
    "study = run_hpo_with_train(\n",
    "     n_trials=4,\n",
    "     epochs=20, patience=8,\n",
    "     take_train_frac=0.10,   # 10% of train only\n",
    "     take_val_fraction=0.20, # 20% of val only (speed)\n",
    "     num_workers=8,\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09398224-0470-46ed-8c25-b48dad4ba85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% HPO visualization utilities (save figures/HTML and a compact JSON report)\n",
    "import os, csv, json, math\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "def trials_df(study: optuna.Study) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for t in study.trials:\n",
    "        if t.state != TrialState.COMPLETE:\n",
    "            continue\n",
    "        r = dict(t.params)\n",
    "        r.update({\n",
    "            \"trial\": t.number,\n",
    "            \"val_auc\": t.value,\n",
    "            \"duration_sec\": t.user_attrs.get(\"duration_sec\", None),\n",
    "            \"save_dir\": t.user_attrs.get(\"save_dir\", None),\n",
    "        })\n",
    "        rows.append(r)\n",
    "    return pd.DataFrame(rows) if rows else pd.DataFrame(\n",
    "        columns=[\"trial\",\"val_auc\",\"duration_sec\",\"save_dir\",\"arch\",\"lr\",\"batch_size\",\"weight_decay\",\"drop_path\"]\n",
    "    )\n",
    "\n",
    "def _safe_title(s: str) -> str:\n",
    "    return \"\".join(c if c.isalnum() or c in \"._- \" else \"_\" for c in s)\n",
    "\n",
    "def plot_opt_history_matplotlib(df: pd.DataFrame, out_path: str):\n",
    "    plt.figure()\n",
    "    x = df[\"trial\"].values\n",
    "    y = df[\"val_auc\"].values\n",
    "    order = np.argsort(x)\n",
    "    plt.plot(x[order], y[order], marker=\"o\")\n",
    "    plt.xlabel(\"Trial\")\n",
    "    plt.ylabel(\"Validation AUC\")\n",
    "    plt.title(\"Optimization History (matplotlib)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=180)\n",
    "    plt.close()\n",
    "\n",
    "def plot_box_auc_by_arch(df: pd.DataFrame, out_path: str):\n",
    "    if \"arch\" not in df.columns: return\n",
    "    plt.figure()\n",
    "    groups = [g[\"val_auc\"].values for _, g in df.groupby(\"arch\")]\n",
    "    labels = [k for k, _ in df.groupby(\"arch\")]\n",
    "    plt.boxplot(groups, labels=labels, showmeans=True)\n",
    "    plt.ylabel(\"Validation AUC\")\n",
    "    plt.title(\"AUC distribution by architecture\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=180)\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter_lr_auc_per_arch(df: pd.DataFrame, out_dir: str):\n",
    "    if \"arch\" not in df.columns or \"lr\" not in df.columns: return\n",
    "    for arch, g in df.groupby(\"arch\"):\n",
    "        plt.figure()\n",
    "        # plot per batch_size to see cluster separation; default color cycle is fine\n",
    "        if \"batch_size\" in g.columns:\n",
    "            for b, gb in g.groupby(\"batch_size\"):\n",
    "                plt.scatter(np.log10(gb[\"lr\"].values), gb[\"val_auc\"].values, label=f\"batch={b}\", s=30)\n",
    "            plt.legend()\n",
    "        else:\n",
    "            plt.scatter(np.log10(g[\"lr\"].values), g[\"val_auc\"].values, s=30)\n",
    "        plt.xlabel(\"log10(lr)\")\n",
    "        plt.ylabel(\"Validation AUC\")\n",
    "        plt.title(f\"LR vs AUC ({arch})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, f\"scatter_lr_auc_{_safe_title(arch)}.png\"), dpi=180)\n",
    "        plt.close()\n",
    "\n",
    "def plot_heatmap_arch_batch_best(df: pd.DataFrame, out_path: str):\n",
    "    if \"arch\" not in df.columns or \"batch_size\" not in df.columns: return\n",
    "    pivot = df.pivot_table(index=\"arch\", columns=\"batch_size\", values=\"val_auc\", aggfunc=\"max\")\n",
    "    vals = pivot.values\n",
    "    plt.figure()\n",
    "    im = plt.imshow(vals, aspect=\"auto\")\n",
    "    plt.xticks(ticks=np.arange(pivot.shape[1]), labels=list(pivot.columns))\n",
    "    plt.yticks(ticks=np.arange(pivot.shape[0]), labels=list(pivot.index))\n",
    "    plt.colorbar(im, label=\"Best val AUC\")\n",
    "    plt.title(\"Best AUC by (arch × batch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=180)\n",
    "    plt.close()\n",
    "\n",
    "def plot_topk_learning_curves(study: optuna.Study, out_dir: str, top_k: int = 5):\n",
    "    # Collect (val_auc, save_dir) and draw curves from training_log.csv\n",
    "    pool: List[Tuple[float, str]] = []\n",
    "    for t in study.trials:\n",
    "        if t.state != TrialState.COMPLETE: \n",
    "            continue\n",
    "        sd = t.user_attrs.get(\"save_dir\")\n",
    "        if not sd: \n",
    "            continue\n",
    "        csv_path = os.path.join(sd, \"training_log.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            pool.append((float(t.value), csv_path))\n",
    "    pool.sort(key=lambda x: -x[0])\n",
    "    pool = pool[:top_k]\n",
    "\n",
    "    for rank, (auc, csv_path) in enumerate(pool, 1):\n",
    "        epochs, val_auc = [], []\n",
    "        with open(csv_path, \"r\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    ep = int(row[\"epoch\"])\n",
    "                    va = float(row[\"val_auc\"])\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if math.isfinite(va):\n",
    "                    epochs.append(ep); val_auc.append(va)\n",
    "        if not epochs: \n",
    "            continue\n",
    "        plt.figure()\n",
    "        plt.plot(epochs[:len(val_auc)], val_auc, marker=\"\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Validation AUC\")\n",
    "        plt.title(f\"Val AUC curve (rank {rank}, best={auc:.4f})\")\n",
    "        plt.tight_layout()\n",
    "        base = os.path.basename(os.path.dirname(csv_path))\n",
    "        plt.savefig(os.path.join(out_dir, f\"val_auc_curve_rank{rank}_{_safe_title(base)}.png\"), dpi=180)\n",
    "        plt.close()\n",
    "\n",
    "def export_report(df: pd.DataFrame, out_dir: str, top_k: int = 10):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # Full table\n",
    "    df_sorted = df.sort_values(\"val_auc\", ascending=False)\n",
    "    df_sorted.to_csv(os.path.join(out_dir, \"trials.csv\"), index=False)\n",
    "    # Compact JSON\n",
    "    best = df_sorted.iloc[0].to_dict() if len(df_sorted) else {}\n",
    "    board = df_sorted.head(top_k).to_dict(orient=\"records\")\n",
    "    report = {\"best\": best, \"leaderboard\": board}\n",
    "    with open(os.path.join(out_dir, \"report.json\"), \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "def viz_hpo(study: optuna.Study, out_dir: str = \"./hpo_figs\", top_k_curves: int = 5):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df = trials_df(study)\n",
    "    if df.empty:\n",
    "        print(\"[viz] No completed trials to visualize.\")\n",
    "        return None\n",
    "\n",
    "    # 1) Save a compact report + CSV\n",
    "    export_report(df, out_dir, top_k=10)\n",
    "\n",
    "    # 2) Plotly (if available)\n",
    "    try:\n",
    "        from optuna.visualization import (\n",
    "            plot_optimization_history,\n",
    "            plot_param_importances,\n",
    "            plot_parallel_coordinate,\n",
    "        )\n",
    "        plot_optimization_history(study).write_html(os.path.join(out_dir, \"opt_history.html\"))\n",
    "        plot_param_importances(study).write_html(os.path.join(out_dir, \"param_importance.html\"))\n",
    "        plot_parallel_coordinate(study).write_html(os.path.join(out_dir, \"parallel_coord.html\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[viz] Plotly visuals unavailable or failed: {e}\")\n",
    "\n",
    "    # 3) Matplotlib fallbacks / complementary views\n",
    "    plot_opt_history_matplotlib(df, os.path.join(out_dir, \"opt_history_matplotlib.png\"))\n",
    "    plot_box_auc_by_arch(df, os.path.join(out_dir, \"auc_box_by_arch.png\"))\n",
    "    plot_scatter_lr_auc_per_arch(df, out_dir)\n",
    "    plot_heatmap_arch_batch_best(df, os.path.join(out_dir, \"heatmap_arch_batch_best.png\"))\n",
    "\n",
    "    # 4) Learning curves for top-K trials (reads training_log.csv)\n",
    "    plot_topk_learning_curves(study, out_dir, top_k=top_k_curves)\n",
    "\n",
    "    # Return DataFrame for interactive inspection\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18078c33-3f49-463a-a767-8fdbea079e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = viz_hpo(study, out_dir=\"./hpo_figs\", top_k_curves=5)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
